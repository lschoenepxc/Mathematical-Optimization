# Aufgaben

**Laura Sch√∂ne, Matr.-Nr.: 15480020**

## Aufgabe 1 (+ Bonusaufgabe): Downhill Simplex

Downhill Simplex siehe: 
- `DownhillSimplex.py`, 
- `test_DHSimplex.py`, 
- `project`-Methode in `SetsFromFunctions.py`, 
- Trennung von `Functions` und `DifferentiableFunctions` in `Functions.py`

## Aufgabe 2.2: Skalierungen von Funktionen

Skalierung
-  Flie√ükommazahlen arbeiten am besten im Bereich ‚àí1,1
-  Man sollte durch Skalierung sicherstellen, dass sich alle Zahlen grob in
dem Bereich aufhalten
- Gr√∂√üere Zahlen k√∂nnen zu float.inff√ºhren
- Kleine Zahlen haben weniger Genauigkeit durch wenige Nachkommastellen
- Im maschinellen Lernen
- skaliert man X-Daten auf [‚àí1,1] oder ùëÅ(0,1) (ggf. auch transformieren),
- skaliert man Y-Daten auf [‚àí1,1] oder ùëÅ(0,1) (ggf. auch transformieren) und
- initialisiert man die Modelle, so dass ùëÅ(0,1) (grob) auf ùëÅ(0,1) abbildet

Skalierung in der Optimierung
- Die zul√§ssige Menge ùëã kann man meistens auch weitl√§ufig in [‚àí1,1] ùëë
reinskalieren
- Die Ausgabewerte der Zielfunktion sind oft vorher nicht bekannt.
- Dann raten
- Oder nach ein paar a-prior Auswertungen skalieren
- Oder nach ein paar Schritten neu skalieren

Skalierung in der Praxis
- ùëì ùë†ùëêùëéùëôùëí = ùë°ùëú ‚àò ùëì ‚àò ùë°ùëñ
- Mit ùë°0: ‚Ñù ‚Üí ‚Ñù: ùë• ‚Ü¶ ùëéùë• + ùëè und ùë°ùëñ: ‚Ñùùëë ‚Üí ‚Ñùùëë: ùë• ‚Ü¶ ùê∑ùë• + ùëê
f√ºr ùëé, ùëè ‚àà ‚Ñù, ùê∑ eine reelle Diagonalmatrix und ùëê ‚àà ‚Ñùùëë
- Das ist eine Komposition und die kann man auch so handhaben.
- Wenn die Probleme im Optimierungsalgorithmus liegen, wird man sie oft los
- Wenn die Probleme in der Auswertung von ùëì liegen, dann leider nicht
- Wenn hochqualitative Optimierungsalgorithmen Probleme machen,
dann ist das eine sehr wahrscheinliche Fehlerquelle

--> Still to do????

## Aufgabe 3: SGD mit NNS

SGD siehe:
- `GradientDescent.py`, Methode `StochasticMinimize`
- Test mit NN in `test_NN.py`
- Complexity `ToLoss` in `FeedForwardNN.py`

## Aufgabe 4: Funktionsklassen und Funktionsmultiplikation

Funktionsklassen: sin, cos, tan, exp, log, sqrt, sigmoid, square, cube, arccos, own_function siehe:
- `DifferentiableFunction.py`
- `__mul__`-Methode f√ºr 2 Funktionen, die eindimensional sind in `Function.py`, `DifferentiableFunction.py`
- Test in `test_Functions.py`

## Aufgabe 5: Interfaces f√ºr K√∂rper, Vektorr√§ume und Matrizen (als lineare Abbildungen) nach Curry-Howard

Fields, VectorSpaces, LinearMap, Gauss siehe
- `Field.py`
- `test_Fields.py`

## Aufgabe 6: Profiling BFGS

siehe
- `profiling_BFGS.py`
- `profile_BFGS.png`

## Aufgabe 7: Hochdimensionale Daten in den Optimierungsalgorithmen

#### Geeignete Optimierungsverfahren

BFGS (Broyden-Fletcher-Goldfarb-Shanno)

- Vorteile: BFGS ist ein Quasi-Newton-Verfahren, das die Hesse-Matrix approximiert und somit effizienter als Newton-Verfahren ist. Es eignet sich gut f√ºr hochdimensionale Probleme, da es die zweite Ableitung nicht explizit berechnet.

- Nachteile: BFGS kann bei sehr hohen Dimensionen speicherintensiv werden, da es eine ( n \times n ) Hesse-Matrix speichert.

Bayesian Optimization

- Vorteile: Bayesian Optimization ist besonders n√ºtzlich f√ºr teure Zielfunktionen, da es die Anzahl der Funktionsauswertungen minimiert. Es verwendet probabilistische Modelle (hier: Gaussian Processes), um die Zielfunktion zu approximieren.
- Nachteile: Die Skalierbarkeit von Bayesian Optimization ist begrenzt, da die Komplexit√§t der Gaussian Processes mit der Anzahl der Dimensionen und Datenpunkten zunimmt.

Stochastic Gradient Descent (SGD)

- Vorteile: SGD ist sehr effizient f√ºr hochdimensionale Probleme, insbesondere bei gro√üen Datens√§tzen. Es verwendet stochastische Approximationen des Gradienten, was die Berechnungskosten pro Iteration reduziert.
- Nachteile: SGD kann bei schlecht konditionierten Problemen langsam konvergieren und erfordert sorgf√§ltige Wahl der Hyperparameter (z.B. Lernrate).

Sequential Quadratic Programming (SQP)

- Vorteile: SQP ist ein leistungsf√§higes Verfahren f√ºr nichtlineare Optimierungsprobleme mit Nebenbedingungen. Es l√∂st eine Reihe von quadratischen Unterproblemen, die einfacher zu handhaben sind.
- Nachteile: SQP kann bei sehr hohen Dimensionen speicherintensiv und rechenaufwendig sein.

#### Weniger geeignete Optimierungsverfahren

Downhill Simplex (Nelder-Mead)

- Nachteile: Der Downhill Simplex Algorithmus skaliert schlecht mit der Dimension des Problems, da die Anzahl der Simplex-Ecken mit der Dimension zunimmt. Dies f√ºhrt zu einer exponentiellen Zunahme der Berechnungskosten und kann die Konvergenz verlangsamen.

Line Search

- Nachteile: Line Search Verfahren sind in der Regel f√ºr unidimensionale Optimierungsprobleme konzipiert und erfordern eine geeignete Suchrichtung. In hochdimensionalen R√§umen kann die Wahl der Suchrichtung und die Durchf√ºhrung der Line Search ineffizient sein.

#### Empfehlungen
F√ºr hochdimensionale zul√§ssige Mengen empfehlen sich insbesondere Verfahren wie BFGS, Bayesian Optimization und Stochastic Gradient Descent. Diese Verfahren sind effizienter und skalierbarer als andere Methoden. Es ist jedoch wichtig, die spezifischen Anforderungen und Eigenschaften des Optimierungsproblems zu ber√ºcksichtigen, um das am besten geeignete Verfahren auszuw√§hlen.

#### M√∂gliche Anpassungen
????

BoundedSets in BO? Skalierte Funktionen in BO? Sampling?

## Aufgabe 8: Cholesky in GP

siehe: 
- `GP.py`

## Aufgabe 9: GP Kerne

siehe:
- RBF-Kernel, MaternKernel, LinearKernel in `GP.py`
- Tests in `test_GP.py`

Eine Ableitung wird nicht ben√∂tigt, daher werden die Kernel-Funktionen als lamdas definiert.
Ergebnisse `test_GP_kernel_comparison_linear_data` und `test_GP_kernel_comparison_noisy_exponential_data`:
- RBF und Matern Kerne: konsistente und zuverl√§ssige Vorhersagen sowohl f√ºr lineare als auch f√ºr verrauschte exponentielle Daten. Sie scheinen gut mit nichtlinearen Daten umzugehen.
- Linear Kernel: Dieser Kernel funktioniert gut bei linearen Daten, jedoch schlechte Performance bei verrauschten exponentiellen Daten, was zu stark abweichenden Vorhersagen f√ºhrt.

## Aufgabe 10: BO Schnittstelle f√ºr vorhandene Daten

siehe: `BayesianOptimization.py`

## Aufgabe 11: Varianten von TSP

siehe `aufgabe_TSP.md`

## Bonusaufgabe: DPLL

- `Literal`, `Clause`, `CNF` als Datenstrukturen in `Literal.py`
- `DPLL.py` f√ºr den Algorithmus
- Tests in `test_Literals.py`, `test_DPLL.py`

Vergleichbarkeit zu anderen Implementierungen: `test_DPLL_PySAT.py`
Nicht gegeben. Die Handhabung √ºber die Klassen `Literals`, `Clause` und `CNF` erzeugen einen zu gro√üen Overhead, daher ist die Laufzeit bedeutend h√∂her f√ºr die hier selbst implementierte L√∂sung.



