### Aufgabe 4.2: Implemetiere 10 weitere Funktionen: sin, cos, tan, exp, log, sqrt, sigmoid, heaviside, square, cube
    @classmethod
    def sin(cls, dimension: int) -> IFunction:
        """Returns a sinus function"""
        return cls(name="sinus", domain=AffineSpace(dimension), evaluate=lambda x: np.sin(x))
    
    @classmethod
    def cos(cls, dimension: int) -> IFunction:
        """Returns a cosinus function"""
        return cls(name="cosinus", domain=AffineSpace(dimension), evaluate=lambda x: np.cos(x))
    
    @classmethod
    def tan(cls, dimension: int) -> IFunction:
        """Returns a tan function"""
        return cls(name="tan", domain=AffineSpace(dimension), evaluate=lambda x: np.tan(x))
    
    @classmethod
    def exp(cls, dimension: int) -> IFunction:
        """Returns a exponential function"""
        return cls(name="exp", domain=AffineSpace(dimension), evaluate=lambda x: np.exp(x))
    
    @classmethod
    def log(cls, dimension: int) -> IFunction:
        """Returns a logarithm function"""
        return cls(name="log", domain=AffineSpace(dimension), evaluate=lambda x: np.log(x))
    
    @classmethod
    def sqrt(cls, dimension: int) -> IFunction:
        """Returns a square root function"""
        return cls(name="sqrt", domain=AffineSpace(dimension), evaluate=lambda x: np.sqrt(x))
    
    @classmethod
    def sigmoid(cls, dimension: int) -> IFunction:
        """Returns a sigmoid function"""
        return cls(name="sigmoid", domain=AffineSpace(dimension), evaluate=lambda x: 1/(1+np.exp(-x)))
    
    @classmethod
    def heaviside(cls, dimension: int) -> IFunction:
        """Returns a heaviside function"""
        return cls(name="heaviside", domain=AffineSpace(dimension), evaluate=lambda x: 1*(x>0))
    
    @classmethod
    def square(cls, dimension: int) -> IFunction:
        """Returns a square function"""
        return cls(name="square", domain=AffineSpace(dimension), evaluate=lambda x: x**2)
    
    @classmethod
    def cube(cls, dimension: int) -> IFunction:
        """Returns a cube function"""
        return cls(name="cube", domain=AffineSpace(dimension), evaluate=lambda x: x**3)
    
    ### Ende Aufgabe 4.2
    
    ### Aufgabe 4.3: Implementiere f(x) = (sqrt(cube(x)+2*square(x)-x+1)*exp(sin(square(x))))/(log(square(square(x))+2)+arccos(x/2))
    
    @classmethod
    def arccos(cls, dimension: int) -> IFunction:
        """Returns a arccos function"""
        return cls(name="arccos", domain=AffineSpace(dimension), evaluate=lambda x: np.arccos(x))
    
    @classmethod
    def own_function(cls, dimension: int) -> IFunction:
        """Returns a own function"""
        return cls(name="own_function", domain=AffineSpace(dimension), evaluate=lambda x: (np.sqrt(x**3+2*x**2-x+1)*np.exp(np.sin(x**2)))/(np.log(x**4+2)+np.arccos(x/2)))


class IFunction(object):
    """This interface models Functions from ISet to R^n."""

    def __init__(self, name: str, domain: ISet, output_domain: ISet):
        super().__init__()
        self._name = name
        self._domain = domain
        self._output_domain = output_domain
        self._verified = False

    @abstractmethod
    def evaluate(self, point: np.ndarray) -> np.ndarray:
        """Evaluates the function at point.
        The parameter "point" is a vector in Double^(Domain.ambient_dimension) such that Domain.is_contained(point)=true."""
        pass

    @property
    def domain(self) -> ISet:
        """The domain of the function, i.e., the set of points at which the function can be evaluated."""
        return self._domain

    @property
    def name(self) -> str:
        """The name of the function, might be used for debugging"""
        return self._name
    
    @property
    def output_domain(self) -> ISet:
        """The domain of the function, i.e., the set of points at which the function can be evaluated."""
        return self._output_domain

    @multimethod
    def __add__(self, other: Union[int, float]) -> 'IFunction':
        """Adds the two functions value wise, where the second function is a constant"""
        return Function(
            name="(" + self.name + ") + " + str(other),
            domain=self.domain,
            evaluate=lambda v: self.evaluate(v) + other
        )

    @multimethod
    def __add__(self, other: 'IFunction') -> 'IFunction':
        """Adds the two functions value wise"""
        return Function(
            name="(" + self.name + ") + (" + other.name + ")",
            domain=self.domain.intersect(other.domain),
            evaluate=lambda v: self.evaluate(v) + other.evaluate(v)
        )
        
    def verify(self):
        """Verifies that the function is well defined"""
        if not self._verfiied:
            assert self._output_domain.shape == self.evaluate(self.domain.point()).shape , "Function is not well defined"
            self._verfiied = True


Umgang mit hochdimensionalen zulässigen Mengen
Hochdimensionale zulässige Mengen stellen besondere Herausforderungen für Optimierungsverfahren dar, insbesondere hinsichtlich der Effizienz und der Konvergenz. Hier sind einige Überlegungen und Empfehlungen, welche Optimierungsverfahren sich für hochdimensionale zulässige Mengen eignen und welche nicht:

Geeignete Optimierungsverfahren
BFGS (Broyden-Fletcher-Goldfarb-Shanno)

Vorteile: BFGS ist ein Quasi-Newton-Verfahren, das die Hesse-Matrix approximiert und somit effizienter als Newton-Verfahren ist. Es eignet sich gut für hochdimensionale Probleme, da es die zweite Ableitung nicht explizit berechnet.
Nachteile: BFGS kann bei sehr hohen Dimensionen speicherintensiv werden, da es eine ( n \times n ) Hesse-Matrix speichert.
Bayesian Optimization

Vorteile: Bayesian Optimization ist besonders nützlich für teure Zielfunktionen, da es die Anzahl der Funktionsauswertungen minimiert. Es verwendet probabilistische Modelle (z.B. Gaussian Processes), um die Zielfunktion zu approximieren.
Nachteile: Die Skalierbarkeit von Bayesian Optimization ist begrenzt, da die Komplexität der Gaussian Processes mit der Anzahl der Dimensionen und Datenpunkten zunimmt.
Stochastic Gradient Descent (SGD)

Vorteile: SGD ist sehr effizient für hochdimensionale Probleme, insbesondere bei großen Datensätzen. Es verwendet stochastische Approximationen des Gradienten, was die Berechnungskosten pro Iteration reduziert.
Nachteile: SGD kann bei schlecht konditionierten Problemen langsam konvergieren und erfordert sorgfältige Wahl der Hyperparameter (z.B. Lernrate).
Sequential Quadratic Programming (SQP)

Vorteile: SQP ist ein leistungsfähiges Verfahren für nichtlineare Optimierungsprobleme mit Nebenbedingungen. Es löst eine Reihe von quadratischen Unterproblemen, die einfacher zu handhaben sind.
Nachteile: SQP kann bei sehr hohen Dimensionen speicherintensiv und rechenaufwendig sein.
Weniger geeignete Optimierungsverfahren
Downhill Simplex (Nelder-Mead)

Nachteile: Der Downhill Simplex Algorithmus skaliert schlecht mit der Dimension des Problems, da die Anzahl der Simplex-Ecken mit der Dimension zunimmt. Dies führt zu einer exponentiellen Zunahme der Berechnungskosten und kann die Konvergenz verlangsamen.
Line Search

Nachteile: Line Search Verfahren sind in der Regel für unidimensionale Optimierungsprobleme konzipiert und erfordern eine geeignete Suchrichtung. In hochdimensionalen Räumen kann die Wahl der Suchrichtung und die Durchführung der Line Search ineffizient sein.
Empfehlungen
Für hochdimensionale zulässige Mengen empfehlen sich insbesondere Verfahren wie BFGS, Bayesian Optimization und Stochastic Gradient Descent. Diese Verfahren sind effizienter und skalierbarer als andere Methoden. Es ist jedoch wichtig, die spezifischen Anforderungen und Eigenschaften des Optimierungsproblems zu berücksichtigen, um das am besten geeignete Verfahren auszuwählen.

Beispiel für die Verwendung von BFGS mit hochdimensionalen zulässigen Mengen
Hier ist ein Beispiel, wie Sie BFGS für ein hochdimensionales Optimierungsproblem mit einem BoundedSet verwenden können:

import numpy as np
from Set import AffineSpace
from DifferentiableFunction import DifferentiableFunction
from BFGS import BFGS
from SetsFromFunctions import BoundedSet

def test_bfgs_high_dimensional():
    dimension = 100  # Hochdimensionale Menge
    R = AffineSpace(dimension)
    f = DifferentiableFunction(
        name="x->sum(x^2)", domain=R,
        evaluate=lambda x: np.array([np.sum(x**2)]),
        jacobian=lambda x: 2 * x.reshape(1, -1))
    
    lower_bounds = np.full(dimension, -10)
    upper_bounds = np.full(dimension, 10)
    bounded_set = BoundedSet(lower_bounds=lower_bounds, upper_bounds=upper_bounds)
    
    bfgs = BFGS()
    starting_point = np.random.uniform(-10, 10, dimension)
    result = bfgs.Minimize(f, startingpoint=starting_point)
    
    print("Optimales Ergebnis:", result)

test_bfgs_high_dimensional()


In diesem Beispiel wird BFGS verwendet, um eine hochdimensionale quadratische Funktion zu minimieren, wobei die zulässige Menge durch BoundedSet definiert ist.

linesearch
